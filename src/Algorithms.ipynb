{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "19ebbf56c786aea1e9c6a27c91595744759c69690a1d58643ea26eacb5eafac6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Methods\n",
    "## Environment methods\n",
    "* **generate_course**: Given a grid size, number of possible actions, and number of sand pits, generate a golf course centered around a randomly placed goal hole.\n",
    "\n",
    "* **generate_test_course**: Similar to generate_course, except with a standardized location and size for testing purposes.\n",
    "\n",
    "* **generate_start**: Given a Map, get a random state that is valid which will be used as a starting state.\n",
    "\n",
    "* **in_ellipse / in_rectangle**: Check whether certain points are within our desired shape.\n",
    "\n",
    "## Print methods\n",
    "* **print_course**: state-reward map\n",
    "* **print_V**: value map\n",
    "\n",
    "## Step()\n",
    "parameters:\n",
    "* start position\n",
    "* strength (PUTT or DRIVER)\n",
    "* direction \n",
    "\n",
    "returns:\n",
    "* end position\n",
    "* reward\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Modules.ipynb"
   ]
  },
  {
   "source": [
    "# Variables and Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Variables_Constants.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, S_t, Q, Map  = generate_test_course(WIDTH, HEIGHT, n_actions, NUM_SAND_PITS)\n",
    "print_course(Map)"
   ]
  },
  {
   "source": [
    "# Policy\n",
    "Epsilon-Greedy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state):\n",
    "    be_greedy = np.random.random() < 1 - EPSILON\n",
    "    if be_greedy: \n",
    "        action = np.argmax(Q[state])\n",
    "    else:                            \n",
    "        action = np.random.randint(0, n_actions)\n",
    "    \n",
    "    s = int(action / len(directions))  # 0 is PUTT, 1 is DRIVER\n",
    "    d = action % len(directions)     \n",
    "    return s, d"
   ]
  },
  {
   "source": [
    "# Q-Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar_dir = \"../output/\"\n",
    "# tar_name = \"q_learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for it in iterations:\n",
    "    # f = open(tar_dir + tar_name + \"_\" + str(it) + \".txt\", 'w')\n",
    "    S, S_t, Q, Map  = generate_test_course(WIDTH, HEIGHT, n_actions, NUM_SAND_PITS) \n",
    "    # Loop through 'it' number of episodes\n",
    "    for i in range(it):\n",
    "        state = generate_start(Map)\n",
    "        # Loop for each step of episode\n",
    "        while state not in S_t:\n",
    "            s, d = epsilon_greedy(Q, state)\n",
    "            newState, reward = step(state, strengths[s], directions[d])\n",
    "            Q[state][s * len(directions) + d] = Q[state][s * len(directions) + d] + ALPHA * (reward + GAMMA * max(Q[newState]) - Q[state][s * len(directions) + d])\n",
    "            state = newState \n",
    "    print(\"iteration : \" + str(it))\n",
    "    print_V(Q)\n",
    "    print()\n",
    "    #f.write(\"iteration : \" + str(it) + \"\\n\")\n",
    "    #print_V_to_f(Q, f)\n",
    "    #f.write(\"\\n\")\n",
    "    #f.close()"
   ]
  },
  {
   "source": [
    "# Dyna-Q"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar_dir = \"../output/\"\n",
    "# tar_name = \"dyna_q\"\n",
    "n_planning_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for it in iterations:\n",
    "    # f = open(tar_dir + tar_name + \"_\" + str(it) + \".txt\", 'w')\n",
    "    S, S_t, Q, Map  = generate_test_course(WIDTH, HEIGHT, n_actions, NUM_SAND_PITS) \n",
    "    model = dict()\n",
    "    for s in S:\n",
    "        for a in range(len(strengths)):\n",
    "            for d in range(len(directions)):\n",
    "                model[s,a,d] = None\n",
    "    # Loop for each episode\n",
    "    for i in range(it):\n",
    "        # Get random state and action by policy\n",
    "        state = generate_start(Map)\n",
    "        s, d = epsilon_greedy(Q, state)\n",
    "        # Take action A\n",
    "        state_next, reward = step(state, strengths[s], directions[d])\n",
    "        # Update Q\n",
    "        Q[state][s * len(directions) + d] = Q[state][s * len(directions) + d] + ALPHA * (reward + GAMMA * max(Q[state_next]) - Q[state][s * len(directions) + d])\n",
    "        # Store the transition in your model array\n",
    "        model[state, s, d] = state_next, reward\n",
    "\n",
    "        for k in range(n_planning_steps):\n",
    "            # random previously observed state\n",
    "            state_p = random.choice(S)\n",
    "            while np.min(Q[state_p]) == 0:\n",
    "                state_p = random.choice(S)\n",
    "\n",
    "            # we want to pick a random previously observed action\n",
    "            action_p_idx = random.choice([i for i in range(n_actions) if Q[state][i] != 0])\n",
    "            \n",
    "            s_p = int(action_p_idx / len(directions)) \n",
    "            d_p = action_p_idx % len(directions)\n",
    "\n",
    "            state_next_p, reward_p = step(state_p, strengths[s_p], directions[d_p])\n",
    "            model[state_p, s_p, d_p] = state_next_p, reward_p\n",
    "\n",
    "            Q[state_p][s_p * len(directions) + d_p] = Q[state_p][s_p * len(directions) + d_p] + ALPHA * (reward_p + GAMMA * max(Q[state_next_p]) - Q[state_p][s_p * len(directions) + d_p])\n",
    "    \n",
    "    print(\"iteration : \" + str(it))\n",
    "    print_V(Q)\n",
    "    print()\n",
    "\n",
    "    #f.write(\"iteration : \" + str(it) + \"\\n\")\n",
    "    #print_V_to_f(Q, f)\n",
    "    #f.write(\"\\n\")\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}